# Ollama HTTP endpoint (default local)
OLLAMA_URL=http://localhost:11434

# Any pulled Ollama chat model: llama3, mistral, llama3.1:8b, qwen2.5, etc.
OLLAMA_MODEL=llama3
